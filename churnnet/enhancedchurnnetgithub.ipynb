{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Add\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Conv1D, Activation, Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"Datasetforimplementation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = \"Label Encoder\"\n",
    "# Encoder = \"One-hot Encoder\"\n",
    "# OverSamplingTecnique = \"\"\n",
    "# OverSamplingTecnique = \"SMOTE\"\n",
    "# OverSamplingTecnique = \"SMOTE-Tomek\"\n",
    "OverSamplingTecnique = \"SMOTE-Enn\"\n",
    "filter_size=5\n",
    "number_of_filter=128\n",
    "flatten_layer_exist=True\n",
    "Model_Name=\"SE Block\"\n",
    "# Model_Name=\"Basic Channel Attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Label Encoder\n",
      "Label Encoder Transformation\n",
      "Geography  :  [0 2 1]  =  ['France' 'Spain' 'Germany']\n",
      "Gender  :  [0 1]  =  ['Female' 'Male']\n"
     ]
    }
   ],
   "source": [
    "if Encoder == \"Label Encoder\":\n",
    "  print(\"Applying Label Encoder\")\n",
    "  df_final = df_raw.copy()\n",
    "  le = LabelEncoder()\n",
    "\n",
    "  text_data_features = ['Geography', 'Gender']\n",
    "\n",
    "  print('Label Encoder Transformation')\n",
    "  for i in text_data_features :\n",
    "      df_final[i] = le.fit_transform(df_final[i])\n",
    "      print(i,' : ',df_final[i].unique(),' = ',le.inverse_transform(df_final[i].unique()))\n",
    "\n",
    "\n",
    "\n",
    "  X = df_final.drop(['Exited'], axis=1).copy()\n",
    "  Y = df_final['Exited'].copy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Encoder == \"One-hot Encoder\":\n",
    "  print(\"Applying One-hot Encoder\")\n",
    "\n",
    "  # One-hot encode categorical columns\n",
    "  categorical_columns = ['Geography', 'Gender']\n",
    "\n",
    "  encoder = OneHotEncoder()\n",
    "  encoded_features = encoder.fit_transform(df_raw[categorical_columns]).toarray()\n",
    "\n",
    "  # Combine one-hot encoded features with numerical features\n",
    "  numerical_features = df_raw.drop(categorical_columns + ['Exited'], axis=1)\n",
    "  X = np.hstack((encoded_features, numerical_features))\n",
    "\n",
    "  # Manually encode 'Churn' column\n",
    "  # df_raw['Churn'] = df_raw['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "  # Extract the target variable Y\n",
    "  Y = df_raw['Exited'].values\n",
    "\n",
    "\n",
    "  # Ensure all data is in float format\n",
    "  X = X.astype(float)\n",
    "  Y = Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X)\n",
    "X=X_resampled_scaled\n",
    "Y=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if OverSamplingTecnique == \"SMOTE\":\n",
    "#   print(\"Applying SMOTE\")\n",
    "#   smote = SMOTE()\n",
    "\n",
    "#   X_resampled, y_resampled = smote.fit_resample(X, Y)\n",
    "#   scaler = StandardScaler()\n",
    "#   X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "#   X=X_resampled_scaled\n",
    "#   Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if OverSamplingTecnique == \"SMOTE-Tomek\":\n",
    "#   print(\"Applying SMOTE-Tomek\")\n",
    "\n",
    "#   smote_tomek = SMOTETomek()\n",
    "#   X_resampled, y_resampled = smote_tomek.fit_resample(X, Y)\n",
    "#   scaler = StandardScaler()\n",
    "#   X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "#   X=X_resampled_scaled\n",
    "#   Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE-Enn\n"
     ]
    }
   ],
   "source": [
    "if OverSamplingTecnique == \"SMOTE-Enn\":\n",
    "  print(\"Applying SMOTE-Enn\")\n",
    "\n",
    "  smote_enn = SMOTEENN()\n",
    "  X_resampled, y_resampled = smote_enn.fit_resample(X, Y)\n",
    "  scaler = StandardScaler()\n",
    "  X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "  X=X_resampled_scaled\n",
    "  Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the channel attention layer\n",
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, reduction_ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.fc = tf.keras.layers.Dense(channels // self.reduction_ratio, activation='relu')\n",
    "        self.attention = tf.keras.layers.Dense(channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.reduce_mean(inputs, axis=[1])  # Global average pooling across time dimension\n",
    "        x = self.fc(x)\n",
    "        x = self.attention(x)\n",
    "        x = tf.expand_dims(x, axis=1)  # Add a new dimension for broadcasting\n",
    "        return inputs * x\n",
    "\n",
    "# Define the spatial attention layer\n",
    "class SpatialAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.max_pool = tf.keras.layers.MaxPooling1D(pool_size=3, strides=1, padding='same')\n",
    "        self.avg_pool = tf.keras.layers.AveragePooling1D(pool_size=3, strides=1, padding='same')\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self.conv1d = tf.keras.layers.Conv1D(filters=1, kernel_size=3, padding='same', activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool_out = self.max_pool(inputs)\n",
    "        avg_pool_out = self.avg_pool(inputs)\n",
    "        concat_out = self.concat([max_pool_out, avg_pool_out])\n",
    "        attention_weights = self.conv1d(concat_out)\n",
    "        return inputs * attention_weights\n",
    "\n",
    "\n",
    "# Define the residual block\n",
    "def residual_block(x, filters, kernel_size):\n",
    "    # Save the input tensor\n",
    "    x_shortcut = x\n",
    "\n",
    "    # First convolutional layer\n",
    "    x = tf.keras.layers.Conv1D(filters, kernel_size, activation='relu', padding='same')(x)\n",
    "\n",
    "    # # Second convolutional layer\n",
    "    x = tf.keras.layers.Conv1D(filters, kernel_size, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # # #Third convolution layer\n",
    "    # x = tf.keras.layers.Conv1D(filters, kernel_size, activation='swish', padding='same')(x)\n",
    "    \n",
    "    # Add the shortcut connection\n",
    "    x = Add()([x, x_shortcut])\n",
    "\n",
    "    # Apply ReLU activation\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the channel attention layer for 1D data\n",
    "class Basic_ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=8):\n",
    "        super(Basic_ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, channels = input_shape[1:]\n",
    "        self.shared_layer1 = Conv1D(channels // self.ratio, kernel_size=1, activation='relu', padding='same')\n",
    "        self.shared_layer2 = Conv1D(channels, kernel_size=1, padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = tf.reduce_mean(inputs, axis=1, keepdims=True)\n",
    "        x1 = self.shared_layer1(x1)\n",
    "        x1 = self.shared_layer2(x1)\n",
    "\n",
    "        x2 = tf.reduce_max(inputs, axis=1, keepdims=True)\n",
    "        x2 = self.shared_layer1(x2)\n",
    "        x2 = self.shared_layer2(x2)\n",
    "\n",
    "        attention = tf.add(x1, x2)\n",
    "        attention = Activation(\"sigmoid\")(attention)\n",
    "        output = Multiply()([inputs, attention])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kfold(filter_size, number_of_filter, flatten_layer_exist, Model_Name):\n",
    "  print(\"Applying K-fold\")\n",
    "  print(f\"Applying {number_of_filter} filters of size {filter_size}\")\n",
    "\n",
    "\n",
    "  # Assuming X and Y are your input and target data\n",
    "  # Define the number of folds\n",
    "  num_folds = 10\n",
    "\n",
    "  # Initialize lists to store the evaluation results\n",
    "  accuracy_scores = []\n",
    "  precision_scores = []\n",
    "  recall_scores = []\n",
    "  f1_scores = []\n",
    "  mcc_scores = []\n",
    "  auc_roc_scores = []\n",
    "\n",
    "\n",
    "\n",
    "  # Perform stratified k-fold cross-validation\n",
    "  fold_number = 1  # Initialize the fold number\n",
    "  # skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "  # from sklearn.model_selection import KFold\n",
    "  kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "  for train_index, test_index in kf.split(X):\n",
    "      print(f\"Fold {fold_number}/{num_folds}:\")\n",
    "      # Split the data into training and test sets for the current fold\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      # X_train, X_test = X[train_index], X[test_index]\n",
    "      Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "      \n",
    "      X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "      X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "      # Create the model with attention mechanisms and residual blocks\n",
    "      inputs = tf.keras.Input(shape=(X_train.shape[1], 1))\n",
    "      x = tf.keras.layers.Conv1D(filters=number_of_filter, kernel_size=filter_size, activation='relu')(inputs)\n",
    "      x_res = residual_block(x, number_of_filter, filter_size)  # Apply the first residual block\n",
    "      if Model_Name == \"SE Block\":\n",
    "        print(\"Applying SE Block\")\n",
    "        x = ChannelAttention()(x_res)  # Apply channel attention\n",
    "      else:\n",
    "        print(\"Applying Basic Channel Attenntion\")\n",
    "        x = Basic_ChannelAttention()(x_res)  # Apply channel attention\n",
    "      x = SpatialAttention()(x)  # Apply spatial attention\n",
    "      x = tf.keras.layers.Conv1D(filters=number_of_filter, kernel_size=filter_size, activation='relu', padding='same')(x)\n",
    "      x_res = residual_block(x, number_of_filter, filter_size)  # Apply the second residual block\n",
    "      if Model_Name == \"SE Block\":\n",
    "        print(\"Applying SE Block\")\n",
    "        x = ChannelAttention()(x_res)  # Apply channel attention\n",
    "      else:\n",
    "        print(\"Applying Basic Channel Attenntion\")\n",
    "        x = Basic_ChannelAttention()(x_res)  # Apply channel attention\n",
    "      x = SpatialAttention()(x)  # Apply spatial attention\n",
    "      if flatten_layer_exist==True:\n",
    "        print(\"Applying flatten layer\")\n",
    "        x = tf.keras.layers.Flatten()(x)  # Flatten the output before dense layers\n",
    "      else:\n",
    "        print(\"Applying Globale Max Pooling 1D layer\")\n",
    "        x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "      x = tf.keras.layers.Dropout(0)(x)\n",
    "      x = tf.keras.layers.Dense(number_of_filter, activation='relu')(x)\n",
    "      outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "      model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "      # Compile and train the model\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "      model.compile(optimizer='ADAM', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "      early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "      model.fit(X_train, Y_train, epochs=30, batch_size=32, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "      # Evaluate the model on the test set\n",
    "      Y_pred = model.predict(X_test)\n",
    "      Y_pred_binary = (Y_pred > 0.5).astype(int)\n",
    "\n",
    "      # Calculate evaluation metrics\n",
    "      accuracy = accuracy_score(Y_test, Y_pred_binary)\n",
    "      precision = precision_score(Y_test, Y_pred_binary)\n",
    "      recall = recall_score(Y_test, Y_pred_binary)\n",
    "      f1 = f1_score(Y_test, Y_pred_binary)\n",
    "      mcc = matthews_corrcoef(Y_test, Y_pred_binary)\n",
    "      auc_roc = roc_auc_score(Y_test, Y_pred)\n",
    "\n",
    "      # Append the scores to the respective lists\n",
    "      accuracy_scores.append(accuracy)\n",
    "      precision_scores.append(precision)\n",
    "      recall_scores.append(recall)\n",
    "      f1_scores.append(f1)\n",
    "      mcc_scores.append(mcc)\n",
    "      auc_roc_scores.append(auc_roc)\n",
    "      fold_number += 1\n",
    "\n",
    "  # Calculate the average scores\n",
    "  avg_accuracy = np.mean(accuracy_scores)\n",
    "  avg_precision = np.mean(precision_scores)\n",
    "  avg_recall = np.mean(recall_scores)\n",
    "  avg_f1 = np.mean(f1_scores)\n",
    "   \n",
    "  avg_mcc = np.mean(mcc_scores)\n",
    "  avg_auc_roc = np.mean(auc_roc_scores)\n",
    "\n",
    "  # Print the average scores\n",
    "  print(\"Average Test Accuracy:\", avg_accuracy)\n",
    "  print(\"Average Precision:\", avg_precision)\n",
    "  print(\"Average Recall:\", avg_recall)\n",
    "  print(\"Average F1 Score:\", avg_f1)\n",
    "  print(\"Average MCC:\", avg_mcc)\n",
    "  print(\"Average AUC-ROC:\", avg_auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying K-fold\n",
      "Applying 128 filters of size 5\n",
      "Fold 1/10:\n",
      "Applying SE Block\n",
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.7979 - loss: 0.4312 - val_accuracy: 0.7746 - val_loss: 0.4398\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8717 - loss: 0.2959 - val_accuracy: 0.8948 - val_loss: 0.2550\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8929 - loss: 0.2592 - val_accuracy: 0.9003 - val_loss: 0.2444\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9064 - loss: 0.2247 - val_accuracy: 0.8711 - val_loss: 0.3305\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9088 - loss: 0.2188 - val_accuracy: 0.9008 - val_loss: 0.2336\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9160 - loss: 0.1986 - val_accuracy: 0.8788 - val_loss: 0.2867\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9255 - loss: 0.1798 - val_accuracy: 0.9250 - val_loss: 0.1809\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.9360 - loss: 0.1540 - val_accuracy: 0.9342 - val_loss: 0.1792\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9509 - loss: 0.1275 - val_accuracy: 0.9447 - val_loss: 0.1597\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9582 - loss: 0.1086 - val_accuracy: 0.9342 - val_loss: 0.2086\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9599 - loss: 0.1018 - val_accuracy: 0.9607 - val_loss: 0.1145\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9678 - loss: 0.0850 - val_accuracy: 0.9607 - val_loss: 0.1185\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9713 - loss: 0.0773 - val_accuracy: 0.9502 - val_loss: 0.1248\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.9724 - loss: 0.0721 - val_accuracy: 0.9534 - val_loss: 0.1360\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.9778 - loss: 0.0630 - val_accuracy: 0.9643 - val_loss: 0.1100\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9732 - loss: 0.0724 - val_accuracy: 0.9694 - val_loss: 0.1043\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9867 - loss: 0.0388 - val_accuracy: 0.9730 - val_loss: 0.0885\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9895 - loss: 0.0314 - val_accuracy: 0.9730 - val_loss: 0.0887\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.9871 - loss: 0.0405 - val_accuracy: 0.9534 - val_loss: 0.1477\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9808 - loss: 0.0609 - val_accuracy: 0.9744 - val_loss: 0.1053\n",
      "Epoch 21/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9906 - loss: 0.0290 - val_accuracy: 0.9790 - val_loss: 0.0701\n",
      "Epoch 22/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9882 - loss: 0.0384 - val_accuracy: 0.9799 - val_loss: 0.0756\n",
      "Epoch 23/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9930 - loss: 0.0195 - val_accuracy: 0.9735 - val_loss: 0.1223\n",
      "Epoch 24/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9916 - loss: 0.0280 - val_accuracy: 0.9808 - val_loss: 0.0703\n",
      "Epoch 25/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9934 - loss: 0.0208 - val_accuracy: 0.9776 - val_loss: 0.0682\n",
      "Epoch 26/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9946 - loss: 0.0179 - val_accuracy: 0.9849 - val_loss: 0.0643\n",
      "Epoch 27/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9915 - loss: 0.0248 - val_accuracy: 0.9698 - val_loss: 0.1325\n",
      "Epoch 28/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9931 - loss: 0.0212 - val_accuracy: 0.9822 - val_loss: 0.1054\n",
      "Epoch 29/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9938 - loss: 0.0188 - val_accuracy: 0.9781 - val_loss: 0.0802\n",
      "Epoch 30/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9939 - loss: 0.0208 - val_accuracy: 0.9785 - val_loss: 0.1049\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "Fold 2/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.7874 - loss: 0.4565 - val_accuracy: 0.8491 - val_loss: 0.3649\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.8811 - loss: 0.2793 - val_accuracy: 0.8861 - val_loss: 0.2727\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9010 - loss: 0.2378 - val_accuracy: 0.8980 - val_loss: 0.2622\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9114 - loss: 0.2210 - val_accuracy: 0.9136 - val_loss: 0.2306\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9171 - loss: 0.1996 - val_accuracy: 0.9044 - val_loss: 0.2386\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9310 - loss: 0.1727 - val_accuracy: 0.9406 - val_loss: 0.1535\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9340 - loss: 0.1555 - val_accuracy: 0.9218 - val_loss: 0.2009\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9530 - loss: 0.1260 - val_accuracy: 0.9209 - val_loss: 0.2082\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9509 - loss: 0.1293 - val_accuracy: 0.9236 - val_loss: 0.1978\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9608 - loss: 0.1076 - val_accuracy: 0.9488 - val_loss: 0.1372\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9646 - loss: 0.0885 - val_accuracy: 0.9506 - val_loss: 0.1323\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9712 - loss: 0.0750 - val_accuracy: 0.9465 - val_loss: 0.1508\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9775 - loss: 0.0643 - val_accuracy: 0.9346 - val_loss: 0.1748\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9782 - loss: 0.0610 - val_accuracy: 0.9552 - val_loss: 0.1387\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9836 - loss: 0.0456 - val_accuracy: 0.9684 - val_loss: 0.0914\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9836 - loss: 0.0466 - val_accuracy: 0.9607 - val_loss: 0.1171\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9808 - loss: 0.0496 - val_accuracy: 0.9625 - val_loss: 0.1536\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9873 - loss: 0.0345 - val_accuracy: 0.9675 - val_loss: 0.0996\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9916 - loss: 0.0287 - val_accuracy: 0.9781 - val_loss: 0.0778\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9895 - loss: 0.0317 - val_accuracy: 0.9607 - val_loss: 0.1152\n",
      "Epoch 21/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9864 - loss: 0.0344 - val_accuracy: 0.9689 - val_loss: 0.1256\n",
      "Epoch 22/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9853 - loss: 0.0347 - val_accuracy: 0.9762 - val_loss: 0.1128\n",
      "Epoch 23/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9914 - loss: 0.0222 - val_accuracy: 0.9744 - val_loss: 0.1005\n",
      "Epoch 24/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9938 - loss: 0.0184 - val_accuracy: 0.9817 - val_loss: 0.0846\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
      "Fold 3/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.7968 - loss: 0.4480 - val_accuracy: 0.8624 - val_loss: 0.3124\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8799 - loss: 0.2855 - val_accuracy: 0.8889 - val_loss: 0.2555\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8938 - loss: 0.2503 - val_accuracy: 0.9108 - val_loss: 0.2090\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9017 - loss: 0.2289 - val_accuracy: 0.8935 - val_loss: 0.2603\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9219 - loss: 0.1914 - val_accuracy: 0.9191 - val_loss: 0.2013\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9262 - loss: 0.1791 - val_accuracy: 0.9332 - val_loss: 0.1746\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: 0.1547 - val_accuracy: 0.9465 - val_loss: 0.1655\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9437 - loss: 0.1394 - val_accuracy: 0.9456 - val_loss: 0.1333\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9484 - loss: 0.1281 - val_accuracy: 0.9538 - val_loss: 0.1298\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9645 - loss: 0.0933 - val_accuracy: 0.9442 - val_loss: 0.1599\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9678 - loss: 0.0827 - val_accuracy: 0.9680 - val_loss: 0.0986\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9726 - loss: 0.0748 - val_accuracy: 0.9588 - val_loss: 0.1335\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9791 - loss: 0.0574 - val_accuracy: 0.9689 - val_loss: 0.0977\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9804 - loss: 0.0568 - val_accuracy: 0.9689 - val_loss: 0.1010\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9798 - loss: 0.0506 - val_accuracy: 0.9675 - val_loss: 0.0923\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9786 - loss: 0.0629 - val_accuracy: 0.9511 - val_loss: 0.1732\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9884 - loss: 0.0375 - val_accuracy: 0.9643 - val_loss: 0.1085\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9871 - loss: 0.0349 - val_accuracy: 0.9771 - val_loss: 0.0885\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9903 - loss: 0.0301 - val_accuracy: 0.9675 - val_loss: 0.1155\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9901 - loss: 0.0323 - val_accuracy: 0.9616 - val_loss: 0.1587\n",
      "Epoch 21/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9903 - loss: 0.0289 - val_accuracy: 0.9643 - val_loss: 0.1271\n",
      "Epoch 22/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9909 - loss: 0.0243 - val_accuracy: 0.9785 - val_loss: 0.0898\n",
      "Epoch 23/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9886 - loss: 0.0319 - val_accuracy: 0.9813 - val_loss: 0.0803\n",
      "Epoch 24/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9957 - loss: 0.0158 - val_accuracy: 0.9794 - val_loss: 0.0936\n",
      "Epoch 25/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9946 - loss: 0.0207 - val_accuracy: 0.9726 - val_loss: 0.1057\n",
      "Epoch 26/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9945 - loss: 0.0210 - val_accuracy: 0.9703 - val_loss: 0.1132\n",
      "Epoch 27/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9917 - loss: 0.0242 - val_accuracy: 0.9707 - val_loss: 0.1460\n",
      "Epoch 28/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9924 - loss: 0.0257 - val_accuracy: 0.9803 - val_loss: 0.0845\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "Fold 4/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - accuracy: 0.7920 - loss: 0.4483 - val_accuracy: 0.8610 - val_loss: 0.3190\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8822 - loss: 0.2840 - val_accuracy: 0.9278 - val_loss: 0.1951\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.8965 - loss: 0.2448 - val_accuracy: 0.9076 - val_loss: 0.2419\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9081 - loss: 0.2191 - val_accuracy: 0.9131 - val_loss: 0.1974\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9220 - loss: 0.1910 - val_accuracy: 0.9003 - val_loss: 0.2520\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9312 - loss: 0.1705 - val_accuracy: 0.9182 - val_loss: 0.2274\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9297 - loss: 0.1677 - val_accuracy: 0.9021 - val_loss: 0.2194\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n",
      "Fold 5/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.7896 - loss: 0.4565 - val_accuracy: 0.8660 - val_loss: 0.3543\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.8813 - loss: 0.2907 - val_accuracy: 0.8761 - val_loss: 0.2909\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.8985 - loss: 0.2481 - val_accuracy: 0.8957 - val_loss: 0.2891\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9115 - loss: 0.2166 - val_accuracy: 0.8875 - val_loss: 0.3048\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9197 - loss: 0.1996 - val_accuracy: 0.9108 - val_loss: 0.2241\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9279 - loss: 0.1696 - val_accuracy: 0.8971 - val_loss: 0.2467\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9459 - loss: 0.1416 - val_accuracy: 0.9511 - val_loss: 0.1576\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9490 - loss: 0.1328 - val_accuracy: 0.9396 - val_loss: 0.1822\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9593 - loss: 0.1054 - val_accuracy: 0.9223 - val_loss: 0.2177\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9605 - loss: 0.0989 - val_accuracy: 0.9364 - val_loss: 0.1984\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9652 - loss: 0.0943 - val_accuracy: 0.9497 - val_loss: 0.1774\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9659 - loss: 0.0906 - val_accuracy: 0.9593 - val_loss: 0.1406\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9810 - loss: 0.0570 - val_accuracy: 0.9607 - val_loss: 0.1872\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9781 - loss: 0.0559 - val_accuracy: 0.9625 - val_loss: 0.1324\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9789 - loss: 0.0513 - val_accuracy: 0.9684 - val_loss: 0.1282\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9836 - loss: 0.0425 - val_accuracy: 0.9744 - val_loss: 0.0957\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9880 - loss: 0.0324 - val_accuracy: 0.9694 - val_loss: 0.1375\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9891 - loss: 0.0283 - val_accuracy: 0.9698 - val_loss: 0.1494\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9891 - loss: 0.0320 - val_accuracy: 0.9657 - val_loss: 0.1514\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9896 - loss: 0.0272 - val_accuracy: 0.9749 - val_loss: 0.1117\n",
      "Epoch 21/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9878 - loss: 0.0385 - val_accuracy: 0.9762 - val_loss: 0.1153\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "Fold 6/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.7867 - loss: 0.4452 - val_accuracy: 0.8487 - val_loss: 0.3480\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8802 - loss: 0.2874 - val_accuracy: 0.9072 - val_loss: 0.2120\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8945 - loss: 0.2501 - val_accuracy: 0.8482 - val_loss: 0.3624\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9078 - loss: 0.2208 - val_accuracy: 0.9040 - val_loss: 0.2277\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9159 - loss: 0.1999 - val_accuracy: 0.9223 - val_loss: 0.1864\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9320 - loss: 0.1722 - val_accuracy: 0.9223 - val_loss: 0.2370\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9383 - loss: 0.1512 - val_accuracy: 0.9506 - val_loss: 0.1425\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9404 - loss: 0.1543 - val_accuracy: 0.9442 - val_loss: 0.1759\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9538 - loss: 0.1205 - val_accuracy: 0.9602 - val_loss: 0.1198\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9579 - loss: 0.1096 - val_accuracy: 0.9602 - val_loss: 0.1370\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9672 - loss: 0.0848 - val_accuracy: 0.9236 - val_loss: 0.2271\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9696 - loss: 0.0844 - val_accuracy: 0.9524 - val_loss: 0.1371\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9758 - loss: 0.0652 - val_accuracy: 0.9534 - val_loss: 0.1304\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9737 - loss: 0.0623 - val_accuracy: 0.9648 - val_loss: 0.1165\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9838 - loss: 0.0473 - val_accuracy: 0.9744 - val_loss: 0.1202\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9831 - loss: 0.0489 - val_accuracy: 0.9680 - val_loss: 0.1183\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9856 - loss: 0.0378 - val_accuracy: 0.9735 - val_loss: 0.1048\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9869 - loss: 0.0350 - val_accuracy: 0.9726 - val_loss: 0.1205\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9903 - loss: 0.0295 - val_accuracy: 0.9561 - val_loss: 0.1626\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9884 - loss: 0.0370 - val_accuracy: 0.9657 - val_loss: 0.1347\n",
      "Epoch 21/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9902 - loss: 0.0285 - val_accuracy: 0.9634 - val_loss: 0.1097\n",
      "Epoch 22/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9907 - loss: 0.0269 - val_accuracy: 0.9753 - val_loss: 0.0970\n",
      "Epoch 23/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9915 - loss: 0.0253 - val_accuracy: 0.9739 - val_loss: 0.1159\n",
      "Epoch 24/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9922 - loss: 0.0222 - val_accuracy: 0.9717 - val_loss: 0.0975\n",
      "Epoch 25/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9919 - loss: 0.0225 - val_accuracy: 0.9753 - val_loss: 0.1124\n",
      "Epoch 26/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9898 - loss: 0.0284 - val_accuracy: 0.9776 - val_loss: 0.1210\n",
      "Epoch 27/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.9968 - loss: 0.0126 - val_accuracy: 0.9689 - val_loss: 0.1856\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "Fold 7/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.7976 - loss: 0.4366 - val_accuracy: 0.8381 - val_loss: 0.3606\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.8815 - loss: 0.2801 - val_accuracy: 0.8980 - val_loss: 0.2438\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8982 - loss: 0.2502 - val_accuracy: 0.8651 - val_loss: 0.2986\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.8984 - loss: 0.2394 - val_accuracy: 0.8765 - val_loss: 0.3001\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9201 - loss: 0.2028 - val_accuracy: 0.9127 - val_loss: 0.2183\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9299 - loss: 0.1751 - val_accuracy: 0.9214 - val_loss: 0.2019\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9347 - loss: 0.1651 - val_accuracy: 0.8994 - val_loss: 0.2785\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9445 - loss: 0.1454 - val_accuracy: 0.9406 - val_loss: 0.1725\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9509 - loss: 0.1265 - val_accuracy: 0.9378 - val_loss: 0.1781\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9610 - loss: 0.0998 - val_accuracy: 0.9474 - val_loss: 0.1456\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9617 - loss: 0.0972 - val_accuracy: 0.9246 - val_loss: 0.2195\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9707 - loss: 0.0775 - val_accuracy: 0.9470 - val_loss: 0.1530\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9808 - loss: 0.0605 - val_accuracy: 0.9543 - val_loss: 0.1410\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9787 - loss: 0.0583 - val_accuracy: 0.9657 - val_loss: 0.1057\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9806 - loss: 0.0541 - val_accuracy: 0.9616 - val_loss: 0.1381\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9821 - loss: 0.0509 - val_accuracy: 0.9680 - val_loss: 0.1004\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9830 - loss: 0.0463 - val_accuracy: 0.9575 - val_loss: 0.1205\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9889 - loss: 0.0304 - val_accuracy: 0.9561 - val_loss: 0.1169\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9885 - loss: 0.0340 - val_accuracy: 0.9758 - val_loss: 0.0912\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9904 - loss: 0.0298 - val_accuracy: 0.9657 - val_loss: 0.1049\n",
      "Epoch 21/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9896 - loss: 0.0311 - val_accuracy: 0.9781 - val_loss: 0.0886\n",
      "Epoch 22/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9947 - loss: 0.0179 - val_accuracy: 0.9703 - val_loss: 0.1196\n",
      "Epoch 23/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9917 - loss: 0.0286 - val_accuracy: 0.9803 - val_loss: 0.0925\n",
      "Epoch 24/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9924 - loss: 0.0232 - val_accuracy: 0.9767 - val_loss: 0.0872\n",
      "Epoch 25/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9904 - loss: 0.0247 - val_accuracy: 0.9712 - val_loss: 0.1347\n",
      "Epoch 26/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9904 - loss: 0.0241 - val_accuracy: 0.9730 - val_loss: 0.1203\n",
      "Epoch 27/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9942 - loss: 0.0173 - val_accuracy: 0.9762 - val_loss: 0.1323\n",
      "Epoch 28/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9959 - loss: 0.0187 - val_accuracy: 0.9762 - val_loss: 0.1224\n",
      "Epoch 29/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9959 - loss: 0.0151 - val_accuracy: 0.9707 - val_loss: 0.0861\n",
      "Epoch 30/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9925 - loss: 0.0248 - val_accuracy: 0.9666 - val_loss: 0.1148\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step\n",
      "Fold 8/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.7950 - loss: 0.4405 - val_accuracy: 0.8194 - val_loss: 0.3630\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8788 - loss: 0.2820 - val_accuracy: 0.8912 - val_loss: 0.2736\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8975 - loss: 0.2456 - val_accuracy: 0.8669 - val_loss: 0.2838\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8988 - loss: 0.2298 - val_accuracy: 0.8957 - val_loss: 0.2513\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9173 - loss: 0.2016 - val_accuracy: 0.9287 - val_loss: 0.1856\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9204 - loss: 0.1922 - val_accuracy: 0.9396 - val_loss: 0.1804\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9369 - loss: 0.1595 - val_accuracy: 0.9310 - val_loss: 0.2045\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9418 - loss: 0.1396 - val_accuracy: 0.9433 - val_loss: 0.1996\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9560 - loss: 0.1170 - val_accuracy: 0.9369 - val_loss: 0.2030\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9594 - loss: 0.0993 - val_accuracy: 0.9387 - val_loss: 0.2037\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9679 - loss: 0.0866 - val_accuracy: 0.9492 - val_loss: 0.1792\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9693 - loss: 0.0838 - val_accuracy: 0.9552 - val_loss: 0.1551\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9770 - loss: 0.0589 - val_accuracy: 0.9515 - val_loss: 0.1826\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9779 - loss: 0.0557 - val_accuracy: 0.9511 - val_loss: 0.1566\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9793 - loss: 0.0559 - val_accuracy: 0.9712 - val_loss: 0.1374\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.9861 - loss: 0.0411 - val_accuracy: 0.9588 - val_loss: 0.1423\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9845 - loss: 0.0430 - val_accuracy: 0.9543 - val_loss: 0.2077\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9847 - loss: 0.0404 - val_accuracy: 0.9689 - val_loss: 0.1348\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9894 - loss: 0.0309 - val_accuracy: 0.9744 - val_loss: 0.1205\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9945 - loss: 0.0202 - val_accuracy: 0.9707 - val_loss: 0.1134\n",
      "Epoch 21/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9854 - loss: 0.0364 - val_accuracy: 0.9698 - val_loss: 0.1032\n",
      "Epoch 22/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9918 - loss: 0.0280 - val_accuracy: 0.9735 - val_loss: 0.1042\n",
      "Epoch 23/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9908 - loss: 0.0292 - val_accuracy: 0.9739 - val_loss: 0.1111\n",
      "Epoch 24/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9930 - loss: 0.0218 - val_accuracy: 0.9721 - val_loss: 0.1014\n",
      "Epoch 25/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9904 - loss: 0.0247 - val_accuracy: 0.9831 - val_loss: 0.0945\n",
      "Epoch 26/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9931 - loss: 0.0219 - val_accuracy: 0.9730 - val_loss: 0.1387\n",
      "Epoch 27/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9926 - loss: 0.0254 - val_accuracy: 0.9739 - val_loss: 0.1305\n",
      "Epoch 28/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9929 - loss: 0.0199 - val_accuracy: 0.9785 - val_loss: 0.0984\n",
      "Epoch 29/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9942 - loss: 0.0168 - val_accuracy: 0.9652 - val_loss: 0.1691\n",
      "Epoch 30/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9922 - loss: 0.0289 - val_accuracy: 0.9813 - val_loss: 0.1430\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "Fold 9/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23ms/step - accuracy: 0.7788 - loss: 0.4600 - val_accuracy: 0.8482 - val_loss: 0.3506\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8807 - loss: 0.2793 - val_accuracy: 0.8903 - val_loss: 0.2619\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.8891 - loss: 0.2548 - val_accuracy: 0.8784 - val_loss: 0.2755\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9036 - loss: 0.2313 - val_accuracy: 0.8701 - val_loss: 0.3037\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9208 - loss: 0.1988 - val_accuracy: 0.9310 - val_loss: 0.1895\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9241 - loss: 0.1783 - val_accuracy: 0.9273 - val_loss: 0.1927\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9361 - loss: 0.1575 - val_accuracy: 0.9104 - val_loss: 0.2103\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9464 - loss: 0.1283 - val_accuracy: 0.9310 - val_loss: 0.1868\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9568 - loss: 0.1071 - val_accuracy: 0.9282 - val_loss: 0.1922\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9592 - loss: 0.1019 - val_accuracy: 0.9246 - val_loss: 0.2008\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.9631 - loss: 0.0917 - val_accuracy: 0.9470 - val_loss: 0.1748\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9678 - loss: 0.0797 - val_accuracy: 0.9616 - val_loss: 0.1268\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9766 - loss: 0.0640 - val_accuracy: 0.9588 - val_loss: 0.1342\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9775 - loss: 0.0561 - val_accuracy: 0.9634 - val_loss: 0.1170\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9822 - loss: 0.0522 - val_accuracy: 0.9689 - val_loss: 0.1141\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9875 - loss: 0.0412 - val_accuracy: 0.9566 - val_loss: 0.1428\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9876 - loss: 0.0386 - val_accuracy: 0.9620 - val_loss: 0.1419\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9855 - loss: 0.0422 - val_accuracy: 0.9721 - val_loss: 0.1333\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9883 - loss: 0.0348 - val_accuracy: 0.9625 - val_loss: 0.1367\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9879 - loss: 0.0354 - val_accuracy: 0.9616 - val_loss: 0.1301\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "Fold 10/10:\n",
      "Applying SE Block\n",
      "Applying SE Block\n",
      "Applying flatten layer\n",
      "Epoch 1/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - accuracy: 0.7902 - loss: 0.4492 - val_accuracy: 0.8272 - val_loss: 0.3598\n",
      "Epoch 2/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8751 - loss: 0.2907 - val_accuracy: 0.8660 - val_loss: 0.2967\n",
      "Epoch 3/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8874 - loss: 0.2570 - val_accuracy: 0.9209 - val_loss: 0.2024\n",
      "Epoch 4/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8971 - loss: 0.2384 - val_accuracy: 0.9364 - val_loss: 0.1880\n",
      "Epoch 5/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9141 - loss: 0.2077 - val_accuracy: 0.9191 - val_loss: 0.2115\n",
      "Epoch 6/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9173 - loss: 0.1883 - val_accuracy: 0.9342 - val_loss: 0.1886\n",
      "Epoch 7/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9297 - loss: 0.1656 - val_accuracy: 0.9337 - val_loss: 0.1774\n",
      "Epoch 8/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9393 - loss: 0.1456 - val_accuracy: 0.9369 - val_loss: 0.1753\n",
      "Epoch 9/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9519 - loss: 0.1267 - val_accuracy: 0.9415 - val_loss: 0.1687\n",
      "Epoch 10/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9512 - loss: 0.1199 - val_accuracy: 0.9204 - val_loss: 0.2139\n",
      "Epoch 11/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9631 - loss: 0.0977 - val_accuracy: 0.9300 - val_loss: 0.1922\n",
      "Epoch 12/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9700 - loss: 0.0810 - val_accuracy: 0.9588 - val_loss: 0.1422\n",
      "Epoch 13/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9697 - loss: 0.0760 - val_accuracy: 0.9438 - val_loss: 0.1663\n",
      "Epoch 14/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.9764 - loss: 0.0606 - val_accuracy: 0.9529 - val_loss: 0.1663\n",
      "Epoch 15/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9767 - loss: 0.0603 - val_accuracy: 0.9488 - val_loss: 0.1813\n",
      "Epoch 16/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9788 - loss: 0.0566 - val_accuracy: 0.9588 - val_loss: 0.1358\n",
      "Epoch 17/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9823 - loss: 0.0492 - val_accuracy: 0.9717 - val_loss: 0.1272\n",
      "Epoch 18/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9877 - loss: 0.0384 - val_accuracy: 0.9707 - val_loss: 0.1147\n",
      "Epoch 19/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9878 - loss: 0.0361 - val_accuracy: 0.9662 - val_loss: 0.1140\n",
      "Epoch 20/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9867 - loss: 0.0361 - val_accuracy: 0.9584 - val_loss: 0.1572\n",
      "Epoch 21/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9881 - loss: 0.0310 - val_accuracy: 0.9680 - val_loss: 0.1338\n",
      "Epoch 22/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9912 - loss: 0.0270 - val_accuracy: 0.9598 - val_loss: 0.1245\n",
      "Epoch 23/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9917 - loss: 0.0264 - val_accuracy: 0.9767 - val_loss: 0.0810\n",
      "Epoch 24/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9944 - loss: 0.0186 - val_accuracy: 0.9634 - val_loss: 0.1403\n",
      "Epoch 25/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9871 - loss: 0.0319 - val_accuracy: 0.9767 - val_loss: 0.0882\n",
      "Epoch 26/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9903 - loss: 0.0264 - val_accuracy: 0.9767 - val_loss: 0.1268\n",
      "Epoch 27/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9927 - loss: 0.0219 - val_accuracy: 0.9785 - val_loss: 0.1164\n",
      "Epoch 28/30\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9916 - loss: 0.0254 - val_accuracy: 0.9808 - val_loss: 0.1049\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
      "Average Test Accuracy: 0.9519206649446443\n",
      "Average Precision: 0.9505003836815688\n",
      "Average Recall: 0.9651803101697229\n",
      "Average F1 Score: 0.9577555908847817\n",
      "Average MCC: 0.9021335299198844\n",
      "Average AUC-ROC: 0.9847388721014821\n"
     ]
    }
   ],
   "source": [
    "kfold(filter_size, number_of_filter, flatten_layer_exist, Model_Name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
