{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Add\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Conv1D, Activation, Multiply\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"Datasetforimplementation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = \"Label Encoder\"\n",
    "# Encoder = \"One-hot Encoder\"\n",
    "# OverSamplingTecnique = \"\"\n",
    "# OverSamplingTecnique = \"SMOTE\"\n",
    "# OverSamplingTecnique = \"SMOTE-Tomek\"\n",
    "OverSamplingTecnique = \"SMOTE-Enn\"\n",
    "filter_size=5\n",
    "number_of_filter=128\n",
    "flatten_layer_exist=True\n",
    "Model_Name=\"SE Block\"\n",
    "# Model_Name=\"Basic Channel Attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Label Encoder\n",
      "Label Encoder Transformation\n",
      "Geography  :  [0 2 1]  =  ['France' 'Spain' 'Germany']\n",
      "Gender  :  [0 1]  =  ['Female' 'Male']\n"
     ]
    }
   ],
   "source": [
    "if Encoder == \"Label Encoder\":\n",
    "  print(\"Applying Label Encoder\")\n",
    "  df_final = df_raw.copy()\n",
    "  le = LabelEncoder()\n",
    "\n",
    "  text_data_features = ['Geography', 'Gender']\n",
    "\n",
    "  print('Label Encoder Transformation')\n",
    "  for i in text_data_features :\n",
    "      df_final[i] = le.fit_transform(df_final[i])\n",
    "      print(i,' : ',df_final[i].unique(),' = ',le.inverse_transform(df_final[i].unique()))\n",
    "\n",
    "\n",
    "\n",
    "  X = df_final.drop(['Exited'], axis=1).copy()\n",
    "  Y = df_final['Exited'].copy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Encoder == \"One-hot Encoder\":\n",
    "  print(\"Applying One-hot Encoder\")\n",
    "\n",
    "  # One-hot encode categorical columns\n",
    "  categorical_columns = ['Geography', 'Gender']\n",
    "\n",
    "  encoder = OneHotEncoder()\n",
    "  encoded_features = encoder.fit_transform(df_raw[categorical_columns]).toarray()\n",
    "\n",
    "  # Combine one-hot encoded features with numerical features\n",
    "  numerical_features = df_raw.drop(categorical_columns + ['Exited'], axis=1)\n",
    "  X = np.hstack((encoded_features, numerical_features))\n",
    "\n",
    "  # Manually encode 'Churn' column\n",
    "  # df_raw['Churn'] = df_raw['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "  # Extract the target variable Y\n",
    "  Y = df_raw['Exited'].values\n",
    "\n",
    "\n",
    "  # Ensure all data is in float format\n",
    "  X = X.astype(float)\n",
    "  Y = Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X)\n",
    "X=X_resampled_scaled\n",
    "Y=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if OverSamplingTecnique == \"SMOTE\":\n",
    "#   print(\"Applying SMOTE\")\n",
    "#   smote = SMOTE()\n",
    "\n",
    "#   X_resampled, y_resampled = smote.fit_resample(X, Y)\n",
    "#   scaler = StandardScaler()\n",
    "#   X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "#   X=X_resampled_scaled\n",
    "#   Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if OverSamplingTecnique == \"SMOTE-Tomek\":\n",
    "#   print(\"Applying SMOTE-Tomek\")\n",
    "\n",
    "#   smote_tomek = SMOTETomek()\n",
    "#   X_resampled, y_resampled = smote_tomek.fit_resample(X, Y)\n",
    "#   scaler = StandardScaler()\n",
    "#   X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "#   X=X_resampled_scaled\n",
    "#   Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE-Enn\n"
     ]
    }
   ],
   "source": [
    "if OverSamplingTecnique == \"SMOTE-Enn\":\n",
    "  print(\"Applying SMOTE-Enn\")\n",
    "\n",
    "  smote_enn = SMOTEENN()\n",
    "  X_resampled, y_resampled = smote_enn.fit_resample(X, Y)\n",
    "  scaler = StandardScaler()\n",
    "  X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "  X=X_resampled_scaled\n",
    "  Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_channels // reduction, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:  # 4D input (batch, channels, height, width) -> CNN\n",
    "            batch_size, channels, height, width = x.shape\n",
    "            avg_pool = torch.mean(x, dim=[2, 3])  # Global Average Pooling\n",
    "        elif x.dim() == 3:  # 3D input (batch, channels, length) -> 1D CNN\n",
    "            batch_size, channels, length = x.shape\n",
    "            avg_pool = torch.mean(x, dim=2)  # Global Average Pooling along length\n",
    "        else:\n",
    "            raise ValueError(f\"Expected input to have 3 or 4 dimensions, but got {x.shape}\")\n",
    "\n",
    "        attn_weights = self.fc(avg_pool)\n",
    "        attn_weights = attn_weights.unsqueeze(-1)  # Expand for broadcasting\n",
    "\n",
    "        return x * attn_weights  # Apply attention weights\n",
    "\n",
    "\n",
    "# Define Spatial Attention Layer\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv1d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_pool = torch.max(x, dim=1, keepdim=True)[0]  # Max pooling along channels\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)  # Average pooling along channels\n",
    "        attn_weights = torch.cat([max_pool, avg_pool], dim=1)\n",
    "        attn_weights = self.conv(attn_weights)\n",
    "        attn_weights = torch.sigmoid(attn_weights)\n",
    "        return x * attn_weights\n",
    "\n",
    "\n",
    "# Define Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return self.relu(x + shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Basic Channel Attention\n",
    "class BasicChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio=8):\n",
    "        super(BasicChannelAttention, self).__init__()\n",
    "        self.shared_conv1 = nn.Conv1d(1, channels // reduction_ratio, kernel_size=1, padding=0)\n",
    "        self.shared_conv2 = nn.Conv1d(channels // reduction_ratio, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n",
    "\n",
    "        attn_avg = self.shared_conv2(F.relu(self.shared_conv1(avg_pool)))\n",
    "        attn_max = self.shared_conv2(F.relu(self.shared_conv1(max_pool)))\n",
    "\n",
    "        attention = torch.sigmoid(attn_avg + attn_max)\n",
    "        return x * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_filters, kernel_size, model_name, use_flatten):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, num_filters, kernel_size, padding=kernel_size//2)\n",
    "        self.res_block1 = ResidualBlock(num_filters, kernel_size)\n",
    "        self.res_block2 = ResidualBlock(num_filters, kernel_size)\n",
    "\n",
    "        if model_name == \"SE Block\":\n",
    "            self.channel_attn1 = ChannelAttention(num_filters)\n",
    "            self.channel_attn2 = ChannelAttention(num_filters)\n",
    "        else:\n",
    "            self.channel_attn1 = BasicChannelAttention(num_filters)\n",
    "            self.channel_attn2 = BasicChannelAttention(num_filters)\n",
    "\n",
    "        self.spatial_attn1 = SpatialAttention()\n",
    "        self.spatial_attn2 = SpatialAttention()\n",
    "\n",
    "        self.use_flatten = use_flatten\n",
    "\n",
    "        if use_flatten:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(num_filters * input_size, num_filters),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(num_filters, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(num_filters, num_filters),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(num_filters, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.channel_attn1(x)\n",
    "        x = self.spatial_attn1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.channel_attn2(x)\n",
    "        x = self.spatial_attn2(x)\n",
    "\n",
    "        if self.use_flatten:\n",
    "            x = self.fc(x)\n",
    "        else:\n",
    "            x = self.global_pool(x).squeeze(-1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(X, Y, filter_size, num_filters, use_flatten, model_name, epochs=30, batch_size=32, num_folds=10):\n",
    "    print(\"Applying K-fold\")\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [], \"mcc\": [], \"auc_roc\": []\n",
    "    }\n",
    "\n",
    "    # Ensure X and Y are NumPy arrays\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, Y), 1):\n",
    "        print(f\"Fold {fold}/{num_folds}:\")\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X_train = torch.tensor(X[train_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        Y_train = torch.tensor(Y[train_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        X_test = torch.tensor(X[test_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        Y_test = torch.tensor(Y[test_idx], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train, Y_train)\n",
    "        test_dataset = TensorDataset(X_test, Y_test)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize model\n",
    "        model = AttentionModel(input_size=X_train.shape[2], num_filters=num_filters, kernel_size=filter_size, model_name=model_name, use_flatten=use_flatten)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = loss_fn(outputs, labels.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs).squeeze()\n",
    "                preds = (outputs >= 0.5).float()\n",
    "                y_true.extend(labels.squeeze().tolist())\n",
    "                y_pred.extend(preds.tolist())\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics[\"accuracy\"].append(accuracy_score(y_true, y_pred))\n",
    "        metrics[\"precision\"].append(precision_score(y_true, y_pred))\n",
    "        metrics[\"recall\"].append(recall_score(y_true, y_pred))\n",
    "        metrics[\"f1\"].append(f1_score(y_true, y_pred))\n",
    "        metrics[\"mcc\"].append(matthews_corrcoef(y_true, y_pred))\n",
    "        metrics[\"auc_roc\"].append(roc_auc_score(y_true, y_pred))\n",
    "\n",
    "    # Print average scores\n",
    "    for key, values in metrics.items():\n",
    "        print(f\"Average {key.capitalize()}: {np.mean(values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying K-fold\n",
      "Fold 1/10:\n",
      "Fold 2/10:\n",
      "Fold 3/10:\n",
      "Fold 4/10:\n",
      "Fold 5/10:\n",
      "Fold 6/10:\n",
      "Fold 7/10:\n",
      "Fold 8/10:\n",
      "Fold 9/10:\n",
      "Fold 10/10:\n",
      "Average Accuracy: 0.8711\n",
      "Average Precision: 0.8915\n",
      "Average Recall: 0.8791\n",
      "Average F1: 0.8852\n",
      "Average Mcc: 0.7385\n",
      "Average Auc_roc: 0.8699\n"
     ]
    }
   ],
   "source": [
    "kfold(X, Y, filter_size=3, num_filters=64, use_flatten=True, model_name=\"SE Block\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
